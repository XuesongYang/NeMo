name: megatron_audio_gpt_peft


############ Data ############
data:
  end_string: null
  global_batch_size: 2
  micro_batch_size: 2
  max_seq_length: 2048
  min_seq_length: 1
  sample_rate: 16000

  test_ds:
    manifest_filepath: ??? # Path to a list of JSONL files corresponding to the source data. Data format is identical to train_ds.
    name: null # Names of the corresponding datasets used to log metrics.
    global_batch_size: ${data.global_batch_size}
    micro_batch_size: ${data.micro_batch_size}
    shuffle: False
    num_workers: 0
    pin_memory: True
    max_seq_length: 2048
    min_seq_length: 1
    drop_last: False
    end_string: ${data.end_string}  # don't change, let hydra resolve from saved config
    context_key: ${data.context_key} # don't change, let hydra resolve from saved config
    answer_key: ${data.answer_key} # don't change, let hydra resolve from saved config
    add_eos: ${data.add_eos} # don't change, let hydra resolve from saved config
    add_sep: ${data.add_sep} # don't change, let hydra resolve from saved config
    add_bos: ${data.add_bos} # don't change, let hydra resolve from saved config
    separate_prompt_and_response_with_newline: ${data.separate_prompt_and_response_with_newline}
    write_predictions_to_file: True
    output_file_path_prefix: "preds" # Prefix of the file to write predictions to.
    truncation_field: ${data.truncation_field}  # don't change, let hydra resolve from saved config
    index_mapping_dir: null # Path to a directory to write index mapping files.
    prompt_template: ${data.prompt_template} # don't change, let hydra resolve from saved config
    tokens_to_generate: 512
    log_every_n_steps: 1
    sample_rate: ${data.sample_rate} # don't change, let hydra resolve from saved config
    audio_locator: null # set it to allow multiple audios in a sample, e.g. '|audio|', and use it in the context field of manifest to specify the locations of audios (`audio_filepath` is a list of audios).

    metric:
      name: "bleu" # Name of the evaluation metric to use. Options: ['exact_string_match', 'loss', 'wer', 'bleu', 'rouge']
      average: null # Average the metric over the dataset. Options: ['macro', 'micro']. Works only for 'F1', 'accuracy' etc. Refer to torchmetrics for metrics where this is supported.
      num_classes: null

############ Inference setting ############
inference:
  greedy: True # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 0  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 1.0 # sampling temperature
  all_probs: False  # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
  min_tokens_to_generate: 0  # The minimum length of the sequence to be generated.
  compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
  outfile_path: output.txt
  compute_attention_mask: True

############ Model ############
model:
  freeze_language_model: true
  freeze_speech_model: false
  freeze_modality_adapter: false
  
  llm:
    # pretrained_model: "meta-llama/Llama-3.2-1B-Instruct"
    pretrained_model: "meta-llama/Meta-Llama-3-8B-Instruct"
    _target_: nemo.collections.llm.LlamaModel
    config: 
      _target_: nemo.collections.llm.Llama3Config8B
      # _target_: nemo.collections.llm.Llama32Config1B
    
  speech_encoder:
    pretrained_model: "stt_en_fastconformer_transducer_large"
    target_module: "encoder"

  modality_adapter:
    input_key_from: "d_model"  # attribute of model dim in the speech model
    input_key_to: "feat_in"  # attribute of input dim in the modality adapter
    output_key: "num_classes"  # attrubuite of output dim in the modality adapter
    config:
      _target_: nemo.collections.asr.modules.ConvASRDecoder
      feat_in: -1  # auto-set
      num_classes: -1  # auto-set
      add_blank: false  # don't add blank class

  peft:
    _target_: nemo.collections.llm.peft.LoRA
    dim: 32


############ Optimizer ############
optim:
  _target_: nemo.lightning.MegatronOptimizerModule
  config:
    _target_: megatron.core.optimizer.OptimizerConfig
    optimizer: adam
    lr: 1e-4
    clip_grad: 5.0
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.CosineAnnealingScheduler
    max_steps: ${trainer.max_steps}
    warmup_steps: 250
    constant_steps: 10000
    min_lr: 5e-5


############ Trainer ############
trainer:
  # _target_: nemo.lightning.Trainer
  devices: -1
  accelerator: gpu
  num_nodes: 1
  max_epochs: 1000  # used to keep epoch logging correctly, but training will stop based on max_steps
  max_steps: 1000000 # 1M steps
  log_every_n_steps: 2 # frequency with which training steps are logged 
  val_check_interval: 1.0 # If is an int n > 1, will run val every n training steps, if a float 0.0 - 1.0 will run val every epoch fraction, e.g. 0.25 will run val every quarter epoch
  num_sanity_val_steps: 0
  # use_distributed_sampler: false

strategy:
  _target_: nemo.lightning.MegatronStrategy
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1

callbacks:
  - _target_: nemo.lightning.pytorch.callbacks.ModelCheckpoint
    monitor: "val_loss"
    mode: "min"
    save_last: true
    save_top_k: 1
    save_weights_only: false

plugins:
  _target_: nemo.lightning.MegatronMixedPrecision
  precision: "bf16-mixed"
  autocast_enabled: null

############ AutoResume ############
resume:
  _target_: nemo.collections.speechlm.utils.resume.SLMAutoResume
  # _target_: nemo.lightning.AutoResume
  # restore_config:
  #   _target_: nemo.lightning.RestoreConfig
  #   path: ???
  resume_from_directory: null
  resume_from_path: null
  adapter_path: null
  resume_if_exists: true
  resume_past_end: false
  resume_ignore_no_checkpoint: true


############ Logging ############
logger:
  _target_: nemo.lightning.NeMoLogger
  log_dir: null

