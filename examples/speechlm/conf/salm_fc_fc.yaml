name: megatron_audio_gpt_peft


############ Data ############
data:
  end_string: null
  global_batch_size: 2
  micro_batch_size: 2
  max_seq_length: 2048
  min_seq_length: 1
  sample_rate: 16000

  train_ds:
    # Example of how to specify paths to multiple datasets
    # manifest_filepath:
    #   - /path/to/squad.jsonl
    #   - /path/to/mnli.jsonl
    #   - /path/to/boolq.jsonl
    # Example of how each dataset is formatted
    # {'audio_filepath': 'audio1.wav', 'offset': 0.0, 'duration': 12.3, 'context': 'transcribe this audio', 'answer': 'I have a dream...'}
    # the 'answer' field can also be 'text', and a default 'context' field is added if missing in manigests, so as to work with ASR manifests
    manifest_filepath: ??? # Path to a list of JSONL files corresponding to the source data.
    global_batch_size: ${data.global_batch_size}
    micro_batch_size: ${data.micro_batch_size}
    shuffle: True
    num_workers: 0
    pin_memory: True
    max_seq_length: ${data.max_seq_length}
    min_seq_length: ${data.min_seq_length}
    drop_last: True
    # Notably, the data weights are controlled by either bucketing_weights
    # or concat_sampling_probabilities depending on the dataset type (tar and
    # non-tar).
    # See audio_text_qa_dataset.py for details.
    concat_sampling_probabilities: null # When providing a list of datasets, this arg defines the sampling probabilities from each dataset when strategy='random'
    context_key: 'context'
    answer_key: 'answer'
    add_eos: True
    # add_eos: False
    end_string: ${data.end_string}
    add_sep: False
    add_bos: False
    separate_prompt_and_response_with_newline: False
    truncation_field: "context" # Options: ['context', 'answer']
    index_mapping_dir: null # Path to a directory to write index mapping files.
    prompt_template: "Q: {context}\nA: {answer}" # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
    # ASR configs
    sample_rate: ${data.sample_rate}
    max_duration: 24 # it is set for LibriSpeech, you may need to update it for your dataset
    min_duration: 0.1
    # tarred datasets
    is_tarred: false
    tarred_audio_filepaths: null
    shuffle_n: 2048
    # bucketing params
    bucketing_strategy: "fully_randomized"
    bucketing_batch_size: null
    sample_alpha: null
    audio_locator: null

  validation_ds:
    manifest_filepath: ??? # Path to a list of JSONL files corresponding to the source data. Data format is identical to train_ds.
    global_batch_size: ${data.global_batch_size}
    micro_batch_size: ${data.micro_batch_size}
    shuffle: False
    num_workers: 0
    pin_memory: True
    max_seq_length: ${data.max_seq_length}
    min_seq_length: ${data.min_seq_length}
    drop_last: False
    context_key: ${data.train_ds.context_key}
    answer_key: ${data.train_ds.answer_key}
    add_eos: ${data.train_ds.add_eos}
    end_string: ${data.common.end_string}
    add_sep: ${data.train_ds.add_sep}
    add_bos: ${data.train_ds.add_bos}
    separate_prompt_and_response_with_newline: ${data.train_ds.separate_prompt_and_response_with_newline}
    write_predictions_to_file: False
    output_file_path_prefix: null # Prefix of the file to write predictions to.
    truncation_field: "context" # Options: ['context', 'answer']
    index_mapping_dir: null # Path to a directory to write index mapping files.
    prompt_template: ${data.train_ds.prompt_template} # fstring to use for assistant prompt. Example: "Q: {input}\nA: {output}"
    tokens_to_generate: 128
    # ASR configs
    sample_rate: ${data.common.sample_rate}
    audio_locator: ${data.train_ds.audio_locator}

    log_every_n_steps: 2
    metric:
      name: "loss" # Name of the evaluation metric to use. Options: ['exact_string_match', 'loss', 'wer', 'bleu', 'rouge']
      average: null # Average the metric over the dataset. Options: ['macro', 'micro']. Works only for 'F1', 'accuracy' etc. Refer to torchmetrics for metrics where this is supported.
      num_classes: null

############ Model ############
model:
  freeze_language_model: true
  freeze_speech_model: true
  freeze_modality_adapter: false
  
  llm:
    pretrained_model: "meta-llama/Meta-Llama-3-8B-Instruct"
    _target_: nemo.collections.llm.LlamaModel
    config: 
      _target_: nemo.collections.llm.Llama3Config8B
    
  speech_encoder:
    pretrained_model: "stt_en_fastconformer_transducer_large"
    target_module: "encoder"
    spec_augment_config:
      _target_: nemo.collections.asr.modules.SpectrogramAugmentation
      freq_masks: 2 # set to zero to disable it
      time_masks: 10 # set to zero to disable it
      freq_width: 27
      time_width: 0.05

  modality_adapter:
    input_key_from: "d_model"  # attribute of model dim in the speech model
    input_key_to: "feat_in"  # attribute of input dim in the modality adapter
    output_key: "feat_out"  # attrubuite of output dim in the modality adapter
    config:
      _target_: nemo.collections.asr.modules.ConformerEncoder
      feat_in: -1
      feat_out: -1 # you may set it if you need different output size other than the default d_model
      n_layers: 2
      d_model: 512

      # Sub-sampling parameters
      subsampling: dw_striding # vggnet, striding, stacking or stacking_norm, dw_striding
      subsampling_factor: 8 # must be power of 2 for striding and vggnet
      subsampling_conv_channels: 256 # set to -1 to make it equal to the d_model
      causal_downsampling: false

      # Reduction parameters: Can be used to add another subsampling layer at a given position.
      # Having a 2x reduction will speedup the training and inference speech while keeping similar WER.
      # Adding it at the end will give the best WER while adding it at the beginning will give the best speedup.
      reduction: null # pooling, striding, or null
      reduction_position: null # Encoder block index or -1 for subsampling at the end of encoder
      reduction_factor: 1

      # Feed forward module's params
      ff_expansion_factor: 4

      # Multi-headed Attention Module's params
      self_attention_model: rel_pos # rel_pos or abs_pos
      n_heads: 8 # may need to be lower for smaller d_models
      # [left, right] specifies the number of steps to be seen from left and right of each step in self-attention
      att_context_size: [-1, -1] # -1 means unlimited context
      att_context_style: regular # regular or chunked_limited
      xscaling: true # scales up the input embeddings by sqrt(d_model)
      untie_biases: true # unties the biases of the TransformerXL layers
      pos_emb_max_len: 5000

      # Convolution module's params
      conv_kernel_size: 9
      conv_norm_type: 'batch_norm' # batch_norm or layer_norm or groupnormN (N specifies the number of groups)
      # conv_context_size can be"causal" or a list of two integers while conv_context_size[0]+conv_context_size[1]+1==conv_kernel_size
      # null means [(kernel_size-1)//2, (kernel_size-1)//2], and 'causal' means [(kernel_size-1), 0]
      conv_context_size: null

      ### regularization
      dropout: 0.1 # The dropout used in most of the Conformer Modules
      dropout_pre_encoder: 0.1 # The dropout used before the encoder
      dropout_emb: 0.0 # The dropout used for embeddings
      dropout_att: 0.1 # The dropout for multi-headed attention modules

      # set to non-zero to enable stochastic depth
      stochastic_depth_drop_prob: 0.0
      stochastic_depth_mode: linear  # linear or uniform
      stochastic_depth_start_layer: 1

  peft:
    _target_: nemo.collections.llm.peft.LoRA
    dim: 32


############ Optimizer ############
optim:
  _target_: nemo.lightning.MegatronOptimizerModule
  config:
    _target_: megatron.core.optimizer.OptimizerConfig
    optimizer: adam
    lr: 1e-4
    clip_grad: 1.0
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.CosineAnnealingScheduler
    max_steps: ${trainer.max_steps}
    warmup_steps: 25000
    constant_steps: 10000
    min_lr: 5e-5



############ Trainer ############
trainer:
  devices: -1
  accelerator: gpu
  num_nodes: 1
  max_epochs: 1000  # used to keep epoch logging correctly, but training will stop based on max_steps
  max_steps: 1000000 # 1M steps
  log_every_n_steps: 2 # frequency with which training steps are logged 
  val_check_interval: 6 # If is an int n > 1, will run val every n training steps, if a float 0.0 - 1.0 will run val every epoch fraction, e.g. 0.25 will run val every quarter epoch
  num_sanity_val_steps: 0

strategy:
  _target_: nemo.lightning.MegatronStrategy
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1

callbacks:
  - _target_: nemo.lightning.pytorch.callbacks.ModelCheckpoint
    monitor: "val_loss"
    mode: "min"
    save_last: true
    save_top_k: 1
    save_weights_only: false

plugins:
  _target_: nemo.lightning.MegatronMixedPrecision
  precision: "bf16-mixed"
  autocast_enabled: null

############ AutoResume ############
resume:
  _target_: nemo.collections.speechlm.utils.resume.SLMAutoResume
  # _target_: nemo.lightning.AutoResume
  # restore_config:
  #   _target_: nemo.lightning.RestoreConfig
  #   path: ???
  resume_from_directory: null
  resume_from_path: null
  adapter_path: null
  resume_if_exists: true
  resume_past_end: false
  resume_ignore_no_checkpoint: true


############ Logging ############
logger:
  _target_: nemo.lightning.NeMoLogger
  log_dir: null

