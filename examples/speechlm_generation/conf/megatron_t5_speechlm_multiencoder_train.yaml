name: megatron_t5_speechllm
exp_dir: ???
english_only_model: true
tensor_model_parallel_size: 1

# speech codecs
num_speech_codebooks: 8
seq_pattern: "parallel" # parallel, delay_parallel, flatten

lm_vocab_size: 30_000

virtual_prompt_style: "p-tuning" # one of 'prompt-tuning', 'p-tuning', or 'inference'

max_position_embeddings: 512
############ Data ############
data:
  sample_rate: 24_000
  dataset_type: t5
  encoder_type: "multi_transformer"
  train_ds:
    manifest_filepath: ???  # a list of json files.
    sample_rate: ${sample_rate}
    use_lhotse: false
    encoder_type: ${encoder_type}
    num_speech_codebooks: ${num_speech_codebooks}
    lm_vocab_size: ${lm_vocab_size}
    seq_pattern: ${seq_pattern}
  validation_ds:  # TODO @xueyang: a list of json files. but now only support a single file.
    manifest_filepath: ???
    sample_rate: ${sample_rate}
    use_lhotse: false
    encoder_type: ${encoder_type}

############ Task Templates  ############
tasks:
#  existing_tasks: []
  new_tasks: ["squad"]
  task_templates:
  - taskname: "squad"
    prompt_template: "<|VIRTUAL_PROMPT_0|> {context} {question} {answer}"
    total_virtual_tokens: 3
    virtual_token_splits: [3]
    truncate_field: context
    answer_field: answer

############ Tokenizer ############
#tokenizer:
#  pretrained_model_name: "bert-large-cased",
#  vocab_file: null  # "/pretrained_models/9a77f10c2793465e8e8a3fa5fcbef8b0_vocab.txt"
#  merges_file: null
#  use_fast: false  # whether to use fast HuggingFace tokenizer
#  num_sentinel_tokens: 10_128 # if not set then 39184 - 29056.
tokenizer:
  library: megatron
  type: BertWordPieceCase
  model: null  # specify `override_token_model` here if any.
  vocab_file: null  # specify `override_vocab_file` here if any. For example, T5 TTS applie "/pretrained_models/9a77f10c2793465e8e8a3fa5fcbef8b0_vocab.txt".
  merges_file: null  # TODO @xueyang: remove this and keep it by default inside function. Was not aware of any usage of this file.
  num_sentinel_tokens: 10_128  # specify overrides here, but would keep 39184 - 29056 by default.
  make_vocab_size_divisible_by: 128
  tensor_model_parallel_size: ${tensor_model_parallel_size}
  english_only_model: ${english_only_model}
  phoneme_probability: null  # TODO @xueyang: should refactor this configs with multiple languages setups.
  use_ipa: false
  g2p:
    english:
      _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
      phoneme_dict: "scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt"
      heteronyms: "scripts/tts_dataset_files/heteronyms-052722"
      phoneme_probability: 0.8
      ignore_ambiguous_words: False
      use_chars: True
      use_stresses: True
      grapheme_prefix: ${model.data.grapheme_prefix}
    spanish:
      _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
      phoneme_dict: "scripts/tts_dataset_files/es_ES/es_ES_nv230301.dict"
      phoneme_probability: 0.8
      use_chars: True
      use_stresses: True
      ignore_ambiguous_words: False
      grapheme_prefix: ${model.data.grapheme_prefix}
      locale: "es-ES"
    mandarin:
      _target_: nemo.collections.tts.g2p.models.zh_cn_pinyin.ChineseG2p
      phoneme_dict: "scripts/tts_dataset_files/zh/36finals/ipa_dict_nv23.05.txt"
      word_segmenter: "jieba"
      phoneme_prefix: ""
      phoneme_case: "lower"
      tone_prefix: "#"
      ascii_letter_prefix: ${model.data.grapheme_prefix}
      ascii_letter_case: "upper"
    german:
      _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
      phoneme_dict: "scripts/tts_dataset_files/de/de_nv230119.dict"
      heteronyms: "scripts/tts_dataset_files/de/de_nv230119.heteronym"
      phoneme_probability: 0.8
      ignore_ambiguous_words: False
      use_chars: True
      use_stresses: True
      grapheme_case: mixed
      grapheme_prefix: ${model.data.grapheme_prefix}
      locale: "de-DE"

############ Optimizer ############
optim:
  _target_: nemo.lightning.MegatronOptimizerModule
  config:
    _target_: megatron.core.optimizer.OptimizerConfig
    optimizer: adam   # TODO @xueyang: adamw seems not supported yet, but T5 is ok for that.
    lr: 1e-4
    clip_grad: 1.0
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.CosineAnnealingScheduler
    max_steps: ${trainer.max_steps}
    warmup_steps: 25000
    constant_steps: 10000
    min_lr: 5e-5

############ Trainer ############
strategy:
  _target_: nemo.lightning.MegatronStrategy
  tensor_model_parallel_size: ${tensor_model_parallel_size}
  pipeline_model_parallel_size: 1

callbacks:
  - _target_: nemo.lightning.pytorch.callbacks.ModelCheckpoint  # model_parallel_size is no longer needed.
    monitor: "val_loss"
    save_last: true
    save_top_k: 3
    mode: "min"
    save_weights_only: false
    filename: "megatron_t5_speechllm_tts--{${callbacks[0].monitor}:.3f}-{step}"
    dirpath: "${exp_dir}/${name}"
  - _target_: nemo.utils.exp_manager.TimingCallback
  - _target_: nemo.utils.exp_manager.DeltaTimingCallback

plugins:
  _target_: nemo.lightning.MegatronMixedPrecision
  precision: "bf16-mixed"  # TODO @xueyang: which default precision is applied by default, bf16-mixed or 32?
  autocast_enabled: null  # TODO @xueyang: it is a boolean variable. Steve default as null, should it be False instead?


############ AutoResume ############
resume:
  _target_: nemo.lightning.AutoResume
  # restore_config:
  #   _target_: nemo.lightning.RestoreConfig
  #   path: ???
  resume_from_directory: null
  resume_from_path: null
  adapter_path: null
  resume_if_exists: true
  resume_past_end: false
  resume_ignore_no_checkpoint: true


############ NeMoLogger ############
logger:
  # If an explicit version is not provided and use_datetime_version is False, the directory will change to dir / name
  _target_: nemo.lightning.NeMoLogger
  log_dir: null
  use_datetime_version: False

#name: megatron_t5_speechllm
#
#trainer:
#  devices: 1
#  accelerator: gpu
#  num_nodes: 1
#  precision: 32
#  logger: False
#  enable_checkpointing: False
#  use_distributed_sampler: False
#  max_epochs: -1
#  max_steps: 250000
#  log_every_n_steps: 10
#  val_check_interval: null
#  check_val_every_n_epoch: 1
#
#exp_manager:
#  explicit_log_dir: null
#  exp_dir: null
#  name: ${name}
#  create_wandb_logger: False
#  resume_if_exists: True
#  resume_ignore_no_checkpoint: True
#  create_checkpoint_callback: True
#  checkpoint_callback_params:
#    monitor: val_loss
#    save_top_k: 3
#    mode: min
#    save_nemo_on_train_end: False # Should be false, correct prompt learning model file is saved at model.nemo_path set below
#    filename: "megatron_t5_speechllm_tts--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}"
#    model_parallel_size: ${model.tensor_model_parallel_size}
#    save_best_model: True
#  create_early_stopping_callback: False
#  early_stopping_callback_params:
#    monitor: "val_loss"
#    mode: "min"
#    min_delta: 0.001
#    patience: 10
#    verbose: True
#
#model:
#  seed: 1234
#  nemo_path: ${name}.nemo # .nemo filename/absolute path to where the virtual prompt model parameters will be saved
#  virtual_prompt_style: "p-tuning" # one of 'prompt-tuning', 'p-tuning', or 'inference'
#  tensor_model_parallel_size: 1
#  pipeline_model_parallel_size: 1
#  global_batch_size: 2
#  micro_batch_size: 2 # micro batch size should equal global batch size when pipeline parallel = 1
#  validation_global_batch_size: ${model.global_batch_size}
#  validation_micro_batch_size: ${model.micro_batch_size}
#  validation_drop_last: False
#  report_validation_metric: False
#  validation_metric: accuracy
#  num_speech_tokens: 10112 # Vocabulary size pertaining to speech
#  seq_pattern: "parallel" # parallel, delay_parallel, flatten
#  attn_prior_scaledown_start_step: 10000
#  attn_prior_end_step: 11000
#  return_all_crossattention_probs: True
#  num_cross_attention_heads: 12 # 12 for 220m, 16 for 3b.
#  restore_path: null # Path to an existing p-tuned/prompt tuned .nemo model you wish to add new tasks to or run inference with
#  save_nemo_on_validation_end: True # Saves an inference ready .nemo file every time a checkpoint is saved during training.
#  existing_tasks: []
#  new_tasks: ["squad"]
#  freeze_model: false
#  use_alignment_loss: true
#  codecmodel_type: nemo_codec
#  codecmodel_path: ???
#  english_only_model: true
#  context_conditioning: encoder
#  train_from_scratch: true
#  override_tokenizer_vocab_file: ???
#  use_flash_attention: false
#  lm_vocab_size: 30000
#  enc_output_to_layers: [[0,1,2],[3,4,5,6,7,8]]
#
#  frozen_model:
#    # micro_batch_size: null
#    # global_batch_size: null
#    # megatron_amp_O2: true
#    # seq_length: 512
#    # max_position_embeddings: 512
#    # precision: bf16
#    # Above is overridden in code
#    tensor_model_parallel_size: 1
#    pipeline_model_parallel_size: 1
#    pipeline_model_parallel_split_rank: 0
#    make_vocab_size_divisible_by: 128
#    pre_process: true
#    post_process: true
#    gradient_as_bucket_view: true
#    native_amp_init_scale: 4294967296
#    native_amp_growth_interval: 1000
#    fp16_lm_cross_entropy: false
#    seed: 1234
#    use_cpu_initialization: false
#    apex_transformer_log_level: 30
#    tokenizer:
#      library: megatron
#      type: BertWordPieceCase
#      model: null
#      vocab_file: null
#      merge_file: null
#      # num_sentinel_tokens: 100
#    optim:
#      name: null
#    data:
#      dataset_type: t5
#    encoder:
#      arch: multi_transformer
#      n_transformers: 2
#      bias_activation_fusion: false
#      use_flash_attention: ${model.use_flash_attention}
#      num_layers: 6
#      hidden_size: 768
#      ffn_hidden_size: 2048
#      num_attention_heads: 12
#      init_method_std: 0.015
#      hidden_dropout: 0.1
#      attention_dropout: 0.1
#      kv_channels: 64
#      activation: geglu
#    decoder:
#      arch: transformer
#      bias_activation_fusion: false
#      use_flash_attention: ${model.use_flash_attention}
#      num_layers: 12
#      hidden_size: 768
#      ffn_hidden_size: 2048
#      num_attention_heads: 12
#      init_method_std: 0.015
#      hidden_dropout: 0.1
#      attention_dropout: 0.1
#      kv_channels: 64
#      activation: geglu
#
#  task_templates:
#  - taskname: "squad"
#    prompt_template: "<|VIRTUAL_PROMPT_0|> {context} {question} {answer}"
#    total_virtual_tokens: 3
#    virtual_token_splits: [3]
#    truncate_field: context
#    answer_field: answer
#
#  p_tuning: # P-tuning specific params
#      encoder_type: "mlp" # Either "mlp" or "lstm", mlp is default
#      num_layers: 2 # 2 recommended for MLP, 1 recommended for LSTM, must be at least 2 for mlp
#      dropout: 0.0
#
#  prompt_tuning: # Prompt tunin specific params
#    new_prompt_init_methods: ['text'] # List of 'text' or 'random', should correspond to tasks listed in new tasks
#    new_prompt_init_text: ['some init text goes here'] # some init text if init method is text, or None if init method is random
#
#  data:
#    grapheme_prefix: null
#    train_ds: ???
#    validation_ds: ???
#    max_seq_length: 2048
#    sample_rate: 24000
#    add_eos: true
#    add_bos: false
#    use_attention_prior: true
#    attention_prior_scaling_factor: 0.05
#    cross_attention_epsilon: 0.0
#    decoder_starts_with_pad: False
#    add_eos_to_decoder_output: True
#    add_sentinel_to_input: True
#    ul2_prompt_token: null # <extra_id_s>, <extra_id_r>, <extra_id_x>
#    shuffle: true
#    num_workers: 4
#    pin_memory: true
#    speech_offset: 30128
#    train_task: all
#    num_speech_codebooks: 8
#    codebook_fps: 86
#    context_duration_min: 2.9
#    context_duration_max: 2.9
#    encoder_type: ${model.frozen_model.encoder.arch}
#    g2p:
#      english:
#        _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
#        phoneme_dict: "scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt"
#        heteronyms: "scripts/tts_dataset_files/heteronyms-052722"
#        phoneme_probability: 0.8
#        ignore_ambiguous_words: False
#        use_chars: True
#        use_stresses: True
#        grapheme_prefix: ${model.data.grapheme_prefix}
#      spanish:
#        _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
#        phoneme_dict: "scripts/tts_dataset_files/es_ES/es_ES_nv230301.dict"
#        phoneme_probability: 0.8
#        use_chars: True
#        use_stresses: True
#        ignore_ambiguous_words: False
#        grapheme_prefix: ${model.data.grapheme_prefix}
#        locale: "es-ES"
#      mandarin:
#        _target_: nemo.collections.tts.g2p.models.zh_cn_pinyin.ChineseG2p
#        phoneme_dict: "scripts/tts_dataset_files/zh/36finals/ipa_dict_nv23.05.txt"
#        word_segmenter: "jieba"
#        phoneme_prefix: ""
#        phoneme_case: "lower"
#        tone_prefix: "#"
#        ascii_letter_prefix: ${model.data.grapheme_prefix}
#        ascii_letter_case: "upper"
#      german:
#        _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
#        phoneme_dict: "scripts/tts_dataset_files/de/de_nv230119.dict"
#        heteronyms: "scripts/tts_dataset_files/de/de_nv230119.heteronym"
#        phoneme_probability: 0.8
#        ignore_ambiguous_words: False
#        use_chars: True
#        use_stresses: True
#        grapheme_case: mixed
#        grapheme_prefix: ${model.data.grapheme_prefix}
#        locale: "de-DE"
#
#  optim:
#    name: fused_adam
#    lr: 1e-4
#    weight_decay: 0.01
#    betas:
#    - 0.9
#    - 0.98
#    sched:
#      name: CosineAnnealing
#      warmup_steps: 1000
#      constant_steps: 0
#      min_lr: 1e-5
#      monitor: val_loss
#      reduce_on_plateau: false
